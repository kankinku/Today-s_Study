\아래는 지금까지 합의/결정된 내용을 **개발자 관점의 “프로젝트 설계도(Architecture + Implementation Blueprint)”**로 정리한 버전이야. (도메인 독립, 모듈 분업, SE-VLN은 “안전 행동 계층”으로 고정)

---

## 0. 프로젝트 목표

- 단일 “LLM 에이전트가 다 해먹는” 구조가 아니라,
    
    - **세계 이해(Truth/Explanation/Prediction)** 모듈과
        
    - **경험 기반 안전 행동(SE-VLN)** 모듈을
        
- **분리된 책임**으로 운영하면서, 결과적으로 **실패율을 낮추고(안전성)**, **시간이 지날수록 성능이 좋아지는(자기 진화)** 시스템을 만든다.
    

---

## 1. 전체 아키텍처 개요

### 1.1 구성 레이어

1. **Orchestrator (실행/조율)**
    

- 라우팅, 계획 실행, 에이전트 호출, 평가/롤백
    

2. **Core Agents (3종)**
    

- 기초 에이전트 (Base)
    
- 자기 진화 에이전트 (Self-Evolving)
    
- 집단 다중 에이전트 (Collective / Multi-Agent)
    

3. **Shared Systems (공용 기반 시스템)**
    

- Planning
    
- Skills Search / Self-RAG
    
- SE-VLN (bootstrap 포함)
    
- Ontology (post-hoc abstraction + 승격 규칙)
    
- Theory of Mind(마음이론)
    
- Knowledge-Specific Optimization (지식 고유 최적화)
    

---

## 2. 에이전트 3종 설계

### 2.1 기초 에이전트 (Base Agent)

**역할**

- 사용자 요청을 “작업 그래프”로 변환하고 실행
    
- 도구 호출(스킬)과 지식 조회(Self-RAG) 수행
    
- SE-VLN 경고/제약을 항상 적용한 상태로 행동
    

**핵심 파이프라인**

1. Intent parse → 2) Plan 생성 → 3) Skills/RAG 조회 → 4) 실행 후보 생성 → 5) SE-VLN 안전 필터 → 6) 실행/응답 → 7) 로그/학습 기록
    

---

### 2.2 자기 진화 에이전트 (Self-Evolving Agent)

**역할**

- Base가 남긴 로그를 기반으로
    
    - 계획 템플릿 개선
        
    - 스킬 선택 정책 개선
        
    - SE-VLN 규칙 후보 생성/정제
        
    - 온톨로지 승격 후보 관리
        
- “성능이 좋아지는 루프” 담당
    

**학습 루프**

- 데이터 수집(실행 로그/성공·실패/회복) →
    
- 평가(Evaluator) →
    
- 제안(Optimizer: 정책/프롬프트/룰/스킬 라우팅) →
    
- A/B 또는 그림자 모드(Shadow) 적용 →
    
- 승격/롤백
    

---

### 2.3 집단 다중 에이전트 (Collective / Multi-Agent)

**역할**

- 복잡한 문제를 여러 관점으로 분해하고 합의
    
- Base의 단일 편향을 줄임
    
- 안전성(리스크) 관점/실행 가능성(비용) 관점/정확성 관점 등을 “역할 에이전트”로 분담
    

**운영 방식**

- 역할 예시:
    
    - Planner / Implementer / Verifier / Risk-Guardian(SE-VLN 대리) / Researcher(Self-RAG)
        
- 결과는 “합의 레이어”에서 병합하고, 최종 실행은 Orchestrator가 수행
    

---

## 3. 공용 모듈 설계 (핵심)

### 3.1 Planning 모듈

**출력 포맷을 고정**해야 진화/평가가 가능함.

- Plan = DAG(작업 그래프)
    
    - 노드: step_id, 목적, 입력, 출력, 예상 비용, 도구, 성공 조건
        
    - 엣지: 의존성
        

**추가 요구**

- 각 step에 **검증 조건(Verifier)** 포함
    
- 실행 중 실패 시 **Fallback step**이 있어야 함
    

---

### 3.2 Skills Search + Self-RAG

**역할**

- “어떤 도구/스킬/문서/기억을 써야 하는가”를 결정
    
- 단순 검색이 아니라, “실행 성공률” 기준으로 랭킹
    

**구성**

- Skill Registry (메타데이터: 입력/출력/제약/비용/실패 패턴)
    
- Retrieval Index (문서/로그/온톨로지/규칙 후보)
    
- Router (현재 Task/Context에 맞는 Top-K 선택)
    

---

### 3.3 SE-VLN (bootstrap 포함) — 안전 행동 계층

**정의 고정**

- SE-VLN은 진실을 판단하지 않는다.
    
- **(Context + Signal + Action) → Failure likelihood** 기반으로 행동 실패를 줄인다.
    

**핵심 구성요소**

- Experience Store: (상황, 관측 패턴, 행동, 결과, 회복 여부)
    
- Failure Mode Taxonomy: 실패 유형 분류(도메인 독립)
    
- Policy Hypotheses: 안전 규칙 후보(활성/비활성/버전)
    
- Risk Gate: 실행 후보를 통과/차단/약화(행동 강도 낮춤)
    

**부트스트랩(콜드스타트)**

- 확신 낮음 + 변화 큼 → 보수적
    
- 고비용/고위험 행동 → 증거 임계값 상향
    
- 단일 가설 → 탐색/검증 우선
    

---

### 3.4 Ontology (post-hoc abstraction)

**역할**

- 경험에서 반복되는 안정 관계를 “공식 지식”으로 승격
    
- 완결성보다 실용성/안전성
    

**승격 조건(합의된 기준을 구현 규칙화)**

- 반복 재현성
    
- 행동 기여도(성능/안전 개선)
    
- 낮은 수정 빈도(안정성)
    
- 설명 가능성(상식/합리성)
    
- 리스크 허용성(fallback 가능)
    

**구성**

- Candidate Relations (경험에서 추출된 관계 후보)
    
- Validation Pipeline (통계/반사실/책임/인간 검토)
    
- Ontology Store (승격된 공식 관계)
    

---

### 3.5 마음이론(Theory of Mind)

**역할**

- 사용자/다른 에이전트의 의도, 제약, 선호를 상태로 모델링
    
- 멀티 에이전트 합의에서 특히 중요
    

**ToM State 예시**

- 목표(Goal), 금지사항(Constraints), 선호(Preferences), 위험 성향(Risk profile), 신뢰도(Confidence in system)
    

---

### 3.6 지식 고유 최적화 (Knowledge-Specific Optimization)

**역할**

- “어떤 지식은 어떤 방식으로 인덱싱/요약/검증해야 성능이 좋아지는가”를 학습
    
- 예: 코드/규칙/로그/설계문서/대화기록마다 최적화 방식이 다름
    

---

## 4. 데이터 모델 (최소 스키마)

### 4.1 실행 로그 (Execution Log)

- run_id, timestamp, agent_type
    
- task_intent, plan_dag
    
- steps: {input, tool, output, success, latency, cost}
    
- se_vln: {risk_score, flags, gate_decision, fallback_used}
    
- outcome: {user_satisfied_proxy, error_type, recovery}
    

### 4.2 경험 저장 (Experience Store)

- context_vector / context_text
    
- signal_pattern (time-aware)
    
- action_taken (normalized)
    
- outcome (success/fail + failure_mode)
    
- recovery (가능/불가, 비용)
    
- confidence_at_time
    

### 4.3 규칙 후보 (Policy Hypothesis)

- rule_id, version, status(active/shadow/off)
    
- trigger (context+signal)
    
- action_bias (avoid/prefer/limit)
    
- stats (support, precision proxy, regret, recovery rate)
    

### 4.4 온톨로지 엔티티/관계

- entity_id, type
    
- relation_id, (entity_a, predicate, entity_b)
    
- stability_score, last_updated, provenance(근거 링크/경험)
    

---

## 5. 실행 흐름 (런타임)

1. User Request
    
2. Base Agent → Plan DAG 생성
    
3. Skills Search + Self-RAG 로 step별 리소스 확보
    
4. 후보 행동 생성
    
5. SE-VLN Risk Gate 적용(차단/완화/대체안)
    
6. 실행 및 결과 산출
    
7. 로그/경험 저장
    
8. Self-Evolving Agent가 주기적으로 평가/정책 업데이트
    
9. Ontology 승격 파이프라인이 안정 관계만 공식화
    
10. Collective Agent는 고난도 작업 시 병렬 검토/합의로 품질 상승
    

---

## 6. 개발 단계 로드맵 (현실적인 구현 순서)

### Phase 1: “돌아가는 최소 시스템”

- Orchestrator + Base Agent
    
- Plan DAG 포맷 고정
    
- Skill Registry + 간단 RAG
    
- SE-VLN bootstrap(룰 몇 개 + Risk Gate)
    
- 로그/경험 저장 스키마 구축
    

### Phase 2: “안전성/검증 강화”

- Failure Mode taxonomy 도입
    
- 반사실 비교(규칙 사용/미사용) 평가기
    
- Fallback 정책 정교화
    

### Phase 3: “자기 진화”

- Self-Evolving Agent(평가/제안/그림자 적용)
    
- 룰/프롬프트/스킬 라우팅 버전 관리
    
- 롤백/가드레일
    

### Phase 4: “집단 지능”

- Multi-Agent 역할 분리 + 합의 레이어
    
- ToM State 반영
    
- 고위험 작업에서만 집단 모드 자동 트리거
    

### Phase 5: “온톨로지 승격”

- 관계 후보 추출
    
- 승격 기준 파이프라인 자동화
    
- 인간 검토는 “고위험만 최소”로
    

---

## 7. 구현 팁 (Windows 개발 환경 기준)

- 로컬 저장소(로그/경험/룰/온톨로지)는 초기엔 SQLite 또는 파일 기반(JSONL)로 시작해도 됨
    
- “버전 관리”는 처음부터 넣는 게 좋아(룰/프롬프트/스킬 라우팅이 다 변하므로)
    

---

원하면 다음 단계로, 내가 **(1) 폴더 구조 + 모듈 인터페이스(코드 레벨 계약)** 까지 내려서

- `orchestrator/`, `agents/`, `se_vln/`, `retrieval/`, `ontology/`, `eval/`, `schemas/`  
    같은 형태로 **실제 리포지토리 설계도**를 만들어 줄게.  
    또는 먼저 **Plan DAG / Experience Store / PolicyHypothesis** 3개를 “고정 스키마(JSON)”로 확정해도 된다